# gpu-ai-inference-server
AI Inference Server that takes trained AI models, load them into memory, and executes inference requests efficiently on NVIDIA GPUs. This project utilizes C++, CUDA programming, and Golang.

# TODO LIST
- [ ] Implement the model and inference_manager files
- [ ] Complete the notebook such that the Google Colab runs the server directly on Go without containers
