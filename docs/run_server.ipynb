{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiTY199E_G87",
        "outputId": "53046bfe-0143-4aa1-8479-efef1471c71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpu-ai-inference-server'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 152 (delta 58), reused 124 (delta 34), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (152/152), 12.32 MiB | 7.58 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "#https://medium.com/@lahari.kethinedi/git-tokens-and-token-generation-for-secure-git-push-from-colab-462928cedde4\n",
        "!git config --global user.email \"USER-EMAIL\"\n",
        "!git config --global user.name \"USERNAME\"\n",
        "!git clone https://github.com/Oscar-W-Chen/gpu-ai-inference-server.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd gpu-ai-inference-server/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT3TWdQA_S1x",
        "outputId": "e61e827a-c8c6-4f1e-fb0a-f0a0775ad1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpu-ai-inference-server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout development\n",
        "!git remote set-url origin 'https://Oscar-W-Chen:GITHUB-AUTH-TOKEN@github.com/Oscar-W-Chen/gpu-ai-inference-server.git'\n",
        "!chmod +x scripts/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_nLmDEC_TiB",
        "outputId": "1acd8d3f-9e8b-40cc-e902-1caf14dfb60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'development' set up to track remote branch 'development' from 'origin'.\n",
            "Switched to a new branch 'development'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSCfgvkYDPyV",
        "outputId": "e11968ab-c1b3-4d3e-c53a-8affe560e861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Sun Mar 16 14:35:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x ./server"
      ],
      "metadata": {
        "id": "AxUoqlCzQH6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./scripts/build_server.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amqumNQuhCec",
        "outputId": "09e826ea-4d15-4a5d-840e-9bf069eea3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== GPU AI Inference Server Test ===\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "CUDA device 0: Tesla T4\n",
            "\n",
            "=== Setting up environment ===\n",
            "Running: go version\n",
            "Go is already installed: go version go1.23.6 linux/amd64\n",
            "Installing required packages...\n",
            "Running: apt-get update && apt-get install -y build-essential cmake git pkg-config\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,753 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,674 kB]\n",
            "Fetched 11.4 MB in 4s (2,635 kB/s)\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "pkg-config is already the newest version (0.29.2-1ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "libbz2-dev libpkgconf3 libreadline-dev\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Errors: W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "Creating build directories...\n",
            "Running: mkdir -p build/inference_engine\n",
            "\n",
            "=== Building inference engine ===\n",
            "Running: chmod +x scripts/build_inference_engine.sh\n",
            "Running: ./scripts/build_inference_engine.sh\n",
            "Create build directories...\n",
            "Building inference engine library...\n",
            "-- CUDA version: 12.5\n",
            "-- CUDA libraries: /usr/local/cuda/lib64/libcudart_static.a;Threads::Threads;dl;/usr/lib/x86_64-linux-gnu/librt.a\n",
            "-- CUDA include path: /usr/local/cuda/include\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/gpu-ai-inference-server/build/inference_engine\n",
            "[100%] Built target inference_engine\n",
            "Inference engine library built successfully\n",
            "Building CUDA test program...\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/gpu-ai-inference-server/build/test\n",
            "[100%] Built target cuda_test\n",
            "CUDA test program built successfully\n",
            "Build process completed successfully\n",
            "Errors: CMake Warning (dev) at CMakeLists.txt:10 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "CMake Warning (dev) at CMakeLists.txt:9 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "\n",
            "✅ Inference engine built successfully\n",
            "\n",
            "=== Building Go server ===\n",
            "Using library path: /content/gpu-ai-inference-server/build/inference_engine/lib\n",
            "CGO_LDFLAGS set to: -L/content/gpu-ai-inference-server/build/inference_engine/lib -linference_engine\n",
            "Running: go build -o gpu-ai-inference-server ./server/main.go\n",
            "✅ Go server built successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.google.com/go/go1.23.6.linux-amd64.tar.gz\n",
        "!tar -C /usr/local -xzf go1.23.6.linux-amd64.tar.gz\n",
        "!rm go1.23.6.linux-amd64.tar.gz\n",
        "import os\n",
        "os.environ[\"PATH\"] += \":/usr/local/go/bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXDWFtureBEn",
        "outputId": "09aa135b-f808-4648-c6a0-3b4e55a0c94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-17 18:57:46--  https://dl.google.com/go/go1.23.6.linux-amd64.tar.gz\n",
            "Resolving dl.google.com (dl.google.com)... 74.125.200.93, 74.125.200.91, 74.125.200.136, ...\n",
            "Connecting to dl.google.com (dl.google.com)|74.125.200.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73643139 (70M) [application/x-gzip]\n",
            "Saving to: ‘go1.23.6.linux-amd64.tar.gz’\n",
            "\n",
            "\r          go1.23.6.   0%[                    ]       0  --.-KB/s               \rgo1.23.6.linux-amd6 100%[===================>]  70.23M   428MB/s    in 0.2s    \n",
            "\n",
            "2025-03-17 18:57:46 (428 MB/s) - ‘go1.23.6.linux-amd64.tar.gz’ saved [73643139/73643139]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended: Launching Go server using script\n",
        "!./scripts/run_server.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFZorenvhgl8",
        "outputId": "bc7a681d-52c7-4179-c8e1-a80722b288a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running server ===\n",
            "./gpu-ai-inference-server: error while loading shared libraries: libinference_engine.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternate: Launching the Go server manually\n",
        "print(\"\\n=== Running server ===\")\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = f\"{os.getcwd()}/build/inference_engine/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
        "!NGROK_AUTHTOKEN=\"NGROK_AUTHTOKEN\" ./gpu-ai-inference-server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmN2nylLeObG",
        "outputId": "74dc0b7c-ac85-4d7b-c035-1d48a03fd5b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running server ===\n",
            "2025/03/17 22:04:52 main.go:28: Starting AI Inference Server...\n",
            "2025/03/17 22:04:52 main.go:34: CUDA Available: true\n",
            "2025/03/17 22:04:52 main.go:35: GPU Device Count: 1\n",
            "2025/03/17 22:04:52 main.go:40: Device 0: Device 0: Tesla T4 (Compute Capability 7.5)\n",
            "[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n",
            "\n",
            "[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n",
            " - using env:\texport GIN_MODE=release\n",
            " - using code:\tgin.SetMode(gin.ReleaseMode)\n",
            "\n",
            "[GIN-debug] GET    /health                   --> main.run.func1 (3 handlers)\n",
            "[GIN-debug] GET    /cuda                     --> main.run.func2 (3 handlers)\n",
            "[GIN-debug] GET    /devices                  --> main.run.func3 (3 handlers)\n",
            "2025/03/17 22:04:52 main.go:79: App URL: https://cb89-35-240-177-255.ngrok-free.app\n",
            "[GIN] 2025/03/17 - 22:04:57 |\u001b[90;43m 404 \u001b[0m|         486ns |  164.153.56.217 |\u001b[97;44m GET     \u001b[0m \"/\"\n",
            "[GIN] 2025/03/17 - 22:04:58 |\u001b[90;43m 404 \u001b[0m|         585ns |  164.153.56.217 |\u001b[97;44m GET     \u001b[0m \"/favicon.ico\"\n",
            "[GIN] 2025/03/17 - 22:05:09 |\u001b[97;42m 200 \u001b[0m|       33.12µs |  164.153.56.217 |\u001b[97;44m GET     \u001b[0m \"/health\"\n",
            "[GIN] 2025/03/17 - 22:05:23 |\u001b[97;42m 200 \u001b[0m|      45.403µs |  164.153.56.217 |\u001b[97;44m GET     \u001b[0m \"/cuda\"\n",
            "[GIN] 2025/03/17 - 22:05:33 |\u001b[97;42m 200 \u001b[0m|      253.61µs |  164.153.56.217 |\u001b[97;44m GET     \u001b[0m \"/devices\"\n",
            "2025/03/17 22:05:48 main.go:94: Shutting down server...\n",
            "Server exited\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"server integration test succeeded\"\n",
        "!git push"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmO8n2q_pbzr",
        "outputId": "955129a8-349b-4c16-ef6f-138ec316c82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[development 3dd19dd] server integration test succeeded\n",
            " 5 files changed, 129 insertions(+), 78 deletions(-)\n",
            " rename ai-inf-server => gpu-ai-inference-server (56%)\n",
            " rename scripts/{colab_build_server.py => build_server.py} (97%)\n",
            "Enumerating objects: 15, done.\n",
            "Counting objects: 100% (15/15), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (9/9), done.\n",
            "Writing objects: 100% (9/9), 6.67 MiB | 3.08 MiB/s, done.\n",
            "Total 9 (delta 4), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (4/4), completed with 4 local objects.\u001b[K\n",
            "To https://github.com/Oscar-W-Chen/gpu-ai-inference-server.git\n",
            "   f461e9f..3dd19dd  development -> development\n"
          ]
        }
      ]
    }
  ]
}
