{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiTY199E_G87",
        "outputId": "be6231a3-ed4a-42d3-f00a-96e2e3f1c2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpu-ai-inference-server'...\n",
            "remote: Enumerating objects: 591, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 591 (delta 99), reused 139 (delta 56), pack-reused 382 (from 2)\u001b[K\n",
            "Receiving objects: 100% (591/591), 127.18 MiB | 22.02 MiB/s, done.\n",
            "Resolving deltas: 100% (259/259), done.\n"
          ]
        }
      ],
      "source": [
        "#https://medium.com/@lahari.kethinedi/git-tokens-and-token-generation-for-secure-git-push-from-colab-462928cedde4\n",
        "!git config --global user.email \"EMAIL\"\n",
        "!git config --global user.name \"USER\"\n",
        "!git clone https://github.com/Oscar-W-Chen/gpu-ai-inference-server.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT3TWdQA_S1x",
        "outputId": "14bd3149-a80a-4b74-e719-36fa97beadb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gpu-ai-inference-server\n"
          ]
        }
      ],
      "source": [
        "cd gpu-ai-inference-server/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_nLmDEC_TiB",
        "outputId": "703d0a52-61b3-40b7-edf6-279db61ff3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Branch 'development' set up to track remote branch 'development' from 'origin'.\n",
            "Switched to a new branch 'development'\n"
          ]
        }
      ],
      "source": [
        "!git checkout development\n",
        "!git remote set-url origin 'https://USER:TOKEN@github.com/Oscar-W-Chen/gpu-ai-inference-server.git'\n",
        "!chmod +x scripts/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAhfeXdBC_CG",
        "outputId": "97281a13-2a60-4289-9b1d-2df2b71f90ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Apr  5 01:54:21 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!nvcc -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf9y1Pd0MQNM",
        "outputId": "bbf5294f-dca9-490c-b3b4-9b8ac4ace47c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx, humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-1.21.0\n",
            "Saved ONNX model to /content/gpu-ai-inference-server/models/test_model/1/model.onnx\n",
            "Saved config.json to /content/gpu-ai-inference-server/models/test_model/1/config.json\n",
            "Testing the model with ONNX Runtime...\n",
            "Input: [[-0.01349723 -1.0577109   0.82254493]]\n",
            "Output: [[-0.6017066  1.8522782]]\n",
            "Model test successful!\n",
            "Test model created in /content/gpu-ai-inference-server/models/test_model/1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx onnxruntime numpy\n",
        "!python scripts/create-test-model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amqumNQuhCec",
        "outputId": "8b8d744d-e1f3-4e2b-8155-a7ebad9100bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== GPU AI Inference Server Test ===\n",
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "CUDA device 0: Tesla T4\n",
            "\n",
            "=== Setting up environment ===\n",
            "Running: go version\n",
            "Installing Go...\n",
            "--2025-04-05 01:54:33--  https://dl.google.com/go/go1.23.6.linux-amd64.tar.gz\n",
            "Resolving dl.google.com (dl.google.com)... 142.251.2.93, 142.251.2.136, 142.251.2.91, ...\n",
            "Connecting to dl.google.com (dl.google.com)|142.251.2.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73643139 (70M) [application/x-gzip]\n",
            "Saving to: ‘go1.23.6.linux-amd64.tar.gz’\n",
            "\n",
            "go1.23.6.linux-amd6 100%[===================>]  70.23M   418MB/s    in 0.2s    \n",
            "\n",
            "2025-04-05 01:54:34 (418 MB/s) - ‘go1.23.6.linux-amd64.tar.gz’ saved [73643139/73643139]\n",
            "\n",
            "Running: tar -C /usr/local -xzf go1.23.6.linux-amd64.tar.gz\n",
            "Running: rm go1.23.6.linux-amd64.tar.gz\n",
            "Running: go version\n",
            "go version go1.23.6 linux/amd64\n",
            "Installing required packages...\n",
            "Running: apt-get update && apt-get install -y build-essential cmake git pkg-config\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "pkg-config is already the newest version (0.29.2-1ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.12).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "libbz2-dev libpkgconf3 libreadline-dev\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 42 not upgraded.\n",
            "Errors: W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "Creating build directories...\n",
            "Running: mkdir -p build/inference_engine\n",
            "\n",
            "=== Building inference engine ===\n",
            "Running: chmod +x scripts/build_inference_engine.sh\n",
            "Running: ./scripts/build_inference_engine.sh --run-tests\n",
            "ONNX Runtime installation detected.\n",
            "Creating build directories...\n",
            "Building inference engine library...\n",
            "-- CUDA version: 12.5\n",
            "-- CUDA libraries: /usr/local/cuda/lib64/libcudart_static.a;Threads::Threads;dl;/usr/lib/x86_64-linux-gnu/librt.a\n",
            "-- CUDA include path: /usr/local/cuda/include\n",
            "-- ONNX Runtime path: /usr/local/onnxruntime\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/gpu-ai-inference-server/build/inference_engine\n",
            "[ 16%] Building CXX object CMakeFiles/inference_engine.dir/src/model_repository.cpp.o\n",
            "[ 33%] Linking CXX shared library lib/libinference_engine.so\n",
            "[100%] Built target inference_engine\n",
            "Inference engine library built successfully\n",
            "Building ONNX test program...\n",
            "-- Configuring done (0.0s)\n",
            "-- Generating done (0.0s)\n",
            "-- Build files have been written to: /content/gpu-ai-inference-server/build/test\n",
            "[ 50%] Linking CXX executable onnx_test\n",
            "[ 50%] Linking CXX executable cuda_test\n",
            "[ 75%] Built target cuda_test\n",
            "[100%] Built target onnx_test\n",
            "ONNX test program built successfully\n",
            "Running CUDA tests...\n",
            "Testing CUDA Utilities\n",
            "======================\n",
            "CUDA Available: Yes\n",
            "CUDA Device Count: 1\n",
            "Device 0: Tesla T4 (Compute Capability 7.5)\n",
            "Memory Total: 15095 MB\n",
            "Memory Free: 14992 MB\n",
            "Memory Used: 102 MB\n",
            "\n",
            "Testing Vector Addition\n",
            "=========================\n",
            "Vector addition succeeded\n",
            "Verifying first 5 elements:\n",
            "1 + 1 = 2 ✓\n",
            "1 + 1 = 2 ✓\n",
            "1 + 1 = 2 ✓\n",
            "1 + 1 = 2 ✓\n",
            "1 + 1 = 2 ✓\n",
            "CUDA tests passed successfully!\n",
            "Running ONNX tests with model at /content/gpu-ai-inference-server/models/test_model/1/model.onnx...\n",
            "Testing ONNX model loading and inference with model at: /content/gpu-ai-inference-server/models/test_model/1/model.onnx\n",
            "CUDA available: Yes\n",
            "CUDA device count: 1\n",
            "Device 0: Tesla T4 (Compute Capability 7.5)\n",
            "Memory total: 15095 MB\n",
            "Memory free: 14992 MB\n",
            "Memory used: 102 MB\n",
            "\n",
            "Loading model...\n",
            "Errors: CMake Warning (dev) at CMakeLists.txt:10 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "CMake Warning (dev) at CMakeLists.txt:9 (find_package):\n",
            "  Policy CMP0146 is not set: The FindCUDA module is removed.  Run \"cmake\n",
            "  --help-policy CMP0146\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "Failed to load model: ONNX model file not found: /content/gpu-ai-inference-server/models/test_model/1/model.onnx/model.onnx\n",
            "\n",
            "✅ Inference engine built successfully\n",
            "\n",
            "=== Building Go server ===\n",
            "Using library path: /content/gpu-ai-inference-server/build/inference_engine/lib\n",
            "CGO_LDFLAGS set to: -L/content/gpu-ai-inference-server/build/inference_engine/lib -linference_engine\n",
            "Running: go build -o gpu-ai-inference-server ./server/main.go\n",
            "✅ Go server built successfully\n"
          ]
        }
      ],
      "source": [
        "!./scripts/build_server.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFZorenvhgl8",
        "outputId": "2c4c84af-02fb-49b1-b183-dc1494770b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Running server ===\n",
            "[SERVER] 2025/04/05 01:54:50 Starting AI Inference Server...\n",
            "[SERVER] 2025/04/05 01:54:50 CUDA Available: true, GPU Device Count: 1\n",
            "[SERVER] 2025/04/05 01:54:50 Device 0: Device 0: Tesla T4 (Compute Capability 7.5)\n",
            "[SERVER] 2025/04/05 01:54:50 Server URL: https://197b-34-16-216-98.ngrok-free.app\n",
            "[SERVER] 2025/04/05 01:55:18 Model 'test_model' version '1' is already loaded\n",
            "[SERVER] 2025/04/05 01:55:20 Processing inference request for model: test_model, version: 1\n",
            "[SERVER] 2025/04/05 01:55:20 Inference response sent successfully\n",
            "[SERVER] 2025/04/05 01:56:48 Shutting down server...\n"
          ]
        }
      ],
      "source": [
        "# Recommended: Launching Go server using script\n",
        "!./scripts/run_server.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmO8n2q_pbzr",
        "outputId": "eed038d0-86a0-4c39-e25c-6b4731d29e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[development 6f4008a] add new test model with metadata\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n",
            "Enumerating objects: 11, done.\n",
            "Counting objects: 100% (11/11), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (6/6), 592 bytes | 592.00 KiB/s, done.\n",
            "Total 6 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/Oscar-W-Chen/gpu-ai-inference-server.git\n",
            "   60c83e7..6f4008a  development -> development\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "!git commit -m \"add new test model with metadata\"\n",
        "!git push"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
